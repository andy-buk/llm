# Transformer-based LLM

Generating text by using self-attention and positional embeddings to capture context from a given window.

Inspiration: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

![Transformer Architecture](image.png)