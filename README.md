# Transformer-based LLM

Generating text by using self-attention and positional embeddings to capture context from a given window.

Inspiration: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

![Transformer Architecture](https://github.com/user-attachments/assets/a5a36681-b2b3-4353-abf6-5c475b6024a1)
