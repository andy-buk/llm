# Transformer-based LLM

Generating text by using self-attention and positional embeddings to capture context from a given window.

Training on over 6 million Wikipedia articles.

Inspiration: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

![Validation Loss](https://github.com/user-attachments/assets/510b56c2-c201-42d3-9a9d-17b57665d1c5)

![Transformer Architecture](https://github.com/user-attachments/assets/a5a36681-b2b3-4353-abf6-5c475b6024a1)
